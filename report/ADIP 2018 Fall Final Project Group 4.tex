%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}     % for figure
\usepackage{subcaption}   % for figure
\usepackage{dblfloatfix} 
\usepackage{csvsimple}

\title{\LARGE \bf
ADIP 2018 Fall Project Foreground Extraction- Group 4
}


\author{Chang-Qi Zhang$^{a}$, Chien-Yu Lin$^{b}$ and Chen-Hung Hu$^{c}$ and  \\
Author’s Student ID: 106368002 laboratory: lab107-1 advisor: Lih-Jen Kau$^{a}$\\
Author’s Student ID: 107368003 laboratory: lab107-1 advisor: Lih-Jen Kau$^{b}$ \\
Author’s Student ID: 107368001 laboratory: lab107-1 advisor: Lih-Jen Kau$^{c}$
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The foreground extraction methods from previous group. They either use a heavy 
computation method, Deeplab or manually assign region to achieve human based 
foreground extraction. In this final project we implement a semi-automatic 
foreground extraction software which can automatically generate foreground
keypoints and provide UI for user select background keypoints. 

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Traditional image segmentation algorithms, for example, watershed and grabcut. 
There are differences between these two methods. Watershed initial by assigning both 
foreground and background keypoints, then based on those points to do gradient 
search. On the other hand, grabcut initial by giving foreground rectangle region then use Gaussian 
mixture model, GMM to find foreground region. Due to the watershed algorithm require 
to assign foreground keypoints on the object which you want to extract, we have to 
integrate object detection method to get the region of the foreground object. 
Common object detection, YOLO only give a bounding box of objects. Those bounding 
boxes usually don't fit perfectly with the foreground as we expect. Therefore, 
we propose using Openpose\cite{openpose} to get accurate foreground keypoints on people.  In this 
way, the runtime for our program will be much shorter than using machine learning 
segmentation technology, DeepLab, Mask RCNN, and SegNet.  

\section{PROCEDURE METHOD}
Fig~\ref{fig:sys} shows the system structure for our semi-foreground segmentation method. 
Our system can automatically generate the foreground keypoints when user click on openpose button.
The algorithm under the hood is based on OpenPose\cite{openpose}. For background keypoints, 
user has to use our user interface to select the background keypoints. Once system get both OpenPose 
foreground keypoints and user defined background keypoints, we then apply Foreground/Background 
segmentation algorithm to extract foreground form the input image.
\begin{figure}[h!]
   \centering
   \includegraphics[width=0.9\linewidth]{{"./doc_src/sys"}.png}
   \caption{System structure}
   \label{fig:sys} 
\end{figure}

\subsection{Foreground/Background Segmenter}

\begin{itemize}
   \item Step 1: Use Guassian filtering, with $\sigma = max(image.width, image.height) \times 0.002$;
    \vspace{1em}
   \item Step 2: Get the gradient vector (x y directions) for each channel (r, g, b),
         Get the length (norm) of each gradient vector for each channel.
         Sum length for gradient vector in each channel.
         Add 1 to the sum and divide 1 using this sum.
         Now we got a priority at this point whose value is in the range (0,1].
         For smooth areas the priority value is high, otherwise the priority is low.
         Gradient in x direction.
   \vspace{1em}
   \item Step 3: Given the labeled keypoints (i.e. foreground or background) and priority image.
         \vspace{0.5em}\newline
         \textbf{Initialization:}
         For each neighbor (we use 8-neighbor in this project) of the keypoints.
         Push each neighbor point into priority queque given their priority.
         \vspace{0.5em}\newline
         \textbf{Propagation:}
         Pop the point with the highest priority.
         Point with higher priority would be processed first.
         Push all non-labeled neighbors around this point into priority queue.
         Set the label of this point to the label of the labeled neighbor.
         Stop when priority queue is empty.
         Figure~\ref{fig:mask} is the result mask, with background in color red, foreground in color green.
\end{itemize}
\begin{figure*}[h!]
   \centering
   \begin{subfigure}[t]{0.19\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/blurred"}.png}
      \caption{Gaussian blurred}
      \label{fig:blurred}
   \end{subfigure}
   \begin{subfigure}[t]{0.19\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/gx"}.png}
      \caption{x direction gradient}
      \label{fig:xg}
   \end{subfigure}
   \begin{subfigure}[t]{0.19\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/gy"}.png}
      \caption{y direction gradient}
      \label{fig:yg}
   \end{subfigure}
   \begin{subfigure}[t]{0.19\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/p"}.png}
      \caption{Priority Image, with black = 0, white = 1}
      \label{fig:priority}
   \end{subfigure}
   \begin{subfigure}[t]{0.19\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/kmask"}.png}
      \caption{result mask, with background in color red, foreground in color green.}
      \label{fig:mask}
   \end{subfigure}
   \caption{Foreground/Background Segmente result figures}
   \label{fig:fb-seg}
\end{figure*}

\begin{figure*}[b!]
   \centering
   \begin{subfigure}[t]{0.25\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/op24"}.png}
      \caption{Test set image 24.}
      \label{fig:ts-24}
   \end{subfigure}
   \hspace{1em}
   \begin{subfigure}[t]{0.25\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/op14"}.png}
      \caption{Test set image 14.}
      \label{fig:ts-14}
   \end{subfigure}
   \hspace{1em}
   \begin{subfigure}[t]{0.25\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/op21"}.png}
      \caption{Test set image 21.}
      \label{fig:ts-21}
   \end{subfigure}
   \vspace{1em}
   \newline
   \begin{subfigure}[t]{0.35\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/op27"}.png}
      \caption{Test set image 27.}
      \label{fig:ts-27}
   \end{subfigure}
   \hspace{1em}
   \begin{subfigure}[t]{0.3\linewidth}
      \includegraphics[width=\linewidth]{{"./doc_src/op29"}.png}
      \caption{Test set image 29.}
      \label{fig:ts-29}
   \end{subfigure}
   \caption{Openpose foreground keypoints results}
   \label{fig:openpose-kp}
\end{figure*}

\subsection{Openpose Foreground Keypoints Generator}
Figure~\ref{fig:openopes-net} shows the convolutional architecture of Openpose. In this 
structure, it use multiple stages for the training progress. Each stage is locally 
supervised after compute loss with original using an intermediate loss layer that 
prevents vanishing gradients during training.\cite{openpoose-net}
the model for our program use six stages for training progress. 
\newpage
The results of foreground keypoints extraction show in Figure~\ref{fig:openpose-kp}.
As you can see, the keypoints which generated by OpenPose are pointing precisely. Also, 
it is able to deal with multi people scenario (Figure~\ref{fig:ts-27} and 
Figure~\ref{fig:ts-29}).

\begin{figure*}[!t]
   \centering
   \includegraphics[width=\linewidth]{{"./doc_src/openpose cnn"}.png}
   \caption{Openpose network architecture\cite{openpoose-net}}
   \label{fig:openopes-net} 
\end{figure*}

\newpage

\section{SOFTWARE ARCHITECTURE}
Figure~\ref{fig:software} shows the software structure for our program. Our program use QT as main 
thread, it is able to do file control. In manual mode, it can read the mouse event 
for user defined foreground and background keypoints. With intelligence mode, our 
program will active Openpose to get human body foreground keypoints.
\begin{figure}[h!]
   \centering
   \includegraphics[width=\linewidth]{{"./doc_src/software"}.png}
   \caption{Software structure}
   \label{fig:software}
\end{figure} 

\section{RESULTS AND COMPARE}
For evaluating the testing results, we used Dell Precision 5520 laptop with 8 cores 
Intel Xeon CPU E3-1505M v6 @ 3.00GHz, Quadro M1200 GPU and 32GB RAM. Running on Ubuntu 16.04 operating 
system. 
\newpage
For evaluation the results, we are using following equations~\ref{eq:precision}, 
\ref{eq:recall} and \ref{eq:rmeasure}.
\begin{equation}
   Precision = \frac{True~positives}{True~positives+False~positives}
   \label{eq:precision} 
\end{equation}
\begin{equation}
   Recall = \frac{True~positives}{True~positives+False~negatives}
   \label{eq:recall} 
\end{equation}
\begin{equation}
   F measure = \frac{2\times Precision}{Precision+Recall}
   \label{eq:rmeasure} 
\end{equation}

\begin{table}[!htb]
   \begin{center}
      \begin{tabular}{|l|l|l|l|l|c|}%
         \hline\bfseries IMG & \bfseries Precision & \bfseries Recall & \bfseries F measure & \bfseries MAE & \bfseries Runtime(ms)% specify table head
         \csvreader[head to column names]{./doc_src/bk.csv}{}% use head of csv as column names
         {\\\hline\image & \precision & \recall &\fmeasure& \MAE & \runtime}% specify your coloumns here
         \\\hline
      \end{tabular}
   \end{center}
   \caption{Testing results with test dataset.}
   \label{tab:result}
\end{table}
\newpage
Our results from Table~\ref{tab:result}, precision and recall are not very good compare 
with pure machine learning segmentation technology, DeepLab or Mask RCNN, but our runtime 
is much faster than either DeepLab or Mask RCNN. Typical runtime for DeepLab and Mask RCNN are
approximate 20+ seconds with Nvidia Titan X GPU machine. On the other hand, our method can be running
in real time, if we execute program on the powerful desktop instead running normal laptop.
\vspace{1em}
\newline
Following Figure~\ref{fig:result3}, \ref{fig:result4}, \ref{fig:result20} are our execution
results with test dataset. 
\begin{figure}[h!]
   \centering
   \includegraphics[width=\linewidth]{{"./doc_src/result/3"}.png}
   \caption{GUI result with test image 3}
   \label{fig:result3} 
\end{figure}

\begin{figure}[h!]
   \centering
   \includegraphics[width=\linewidth]{{"./doc_src/result/4"}.png}
   \caption{GUI result with test image 4}
   \label{fig:result4} 
\end{figure}

\begin{figure}[h!]
   \centering
   \includegraphics[width=\linewidth]{{"./doc_src/result/20"}.png}
   \caption{GUI result with test image 20}
   \label{fig:result20} 
\end{figure}

\newpage

\section{PROJECT CONTRIBUTION}
\begin{itemize}
   \item Chang-Qi Zhang$^{a}$: Creating and testing library for Openpose foreground 
   keypoints extraction, PPT author and report author.
   \item Chen-Hung Hu$^{b}$: QT user interface and doing software libraries integration from
   other authors$^{ac}$.
   \item Chien-Yu Lin$^{c}$: implementing foreground/background Segmenter algorism, helping 
   author$^{b}$ to do system integration and project software structure design.
\end{itemize}

\begin{thebibliography}{99}
\bibitem{openpose} Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh, Open{P}ose: realtime multi-person 2{D} pose estimation using {P}art {A}ffinity {F}ields
\bibitem{openpoose-net} Shih-En Wei, Varun Ramakrishna, Takeo Kanade and Yaser Sheikh, Convolutional Pose Machines
\bibitem{} Concurrent computation of topological watershed on shared memory parallel machines
\bibitem{} The Morphological Approach to Segmentation - The Watershed Transformation (S. Beucher and F. Meyer)
\bibitem{} Topographic distance and watershed lines
\bibitem{} Watershed Approaches For Color Image Segmentation 
\bibitem{} Watershed wiki page
\bibitem{} How to code a nice user-guided foreground extraction algorithm? (Addendum)
\end{thebibliography}




\end{document}
